{"/huggingface-tutorial/others/about/": {
    "title": "Conclusion",
    "keywords": "Jekyll",
    "url": "/huggingface-tutorial/others/about/",
    "body": "This guide has introduced you to the powerful tools and capabilities offered by the Hugging Face ecosystem. We’ve explored natural language processing, computer vision, audio processing, multimodal applications, and advanced topics like fine-tuning and deployment. We’ve also looked at domain-specific applications and advanced model techniques to optimize performance. To continue your journey with Hugging Face, here are some next steps: Explore the Model Hub: Browse the thousands of pre-trained models on the Hugging Face Hub to find ones suitable for your specific use cases. Join the Community: Engage with the vibrant community of AI practitioners on the Hugging Face forum and Discord. Contribute: Share your own models, datasets, or Spaces with the community to advance the state of AI. Build Real Applications: Apply what you’ve learned to develop AI-powered applications that solve real-world problems. Resources and Further Learning Documentation: Hugging Face Documentation Transformers Documentation Datasets Documentation Tutorials and Courses: Hugging Face Course NLP with Transformers Book Community: Hugging Face Forums Hugging Face Discord GitHub Issues The field of AI is rapidly evolving, and Hugging Face is at the forefront of making these advanced technologies accessible to everyone. By mastering these tools, you’re well-equipped to participate in this exciting journey. Happy building!"
  },"/huggingface-tutorial/hugging%20face%20ecosystem%20tools/2025-03-09-hugging-face-spaces.html": {
    "title": "Hugging Face Spaces",
    "keywords": "Hugging Face Ecosystem Tools",
    "url": "/huggingface-tutorial/hugging%20face%20ecosystem%20tools/2025-03-09-hugging-face-spaces.html",
    "body": "Spaces allows you to create and share demos of your machine learning models with a simple web interface. Hands-on Example: Creating a Gradio Demo import gradio as gr from transformers import pipeline # Initialize model classifier = pipeline(\"sentiment-analysis\") # Define function for the interface def analyze_sentiment(text): result = classifier(text)[0] return f\"{result['label']} (Confidence: {result['score']:.4f})\" # Create Gradio interface demo = gr.Interface( fn=analyze_sentiment, inputs=gr.Textbox(lines=4, placeholder=\"Enter text to analyze...\"), outputs=\"text\", title=\"Sentiment Analysis Demo\", description=\"This demo uses a pre-trained model to analyze the sentiment of text.\", examples=[ \"I absolutely loved this movie! The acting was superb.\", \"The service at this restaurant was terrible and the food was bland.\", \"The weather today is okay, not great but not terrible either.\" ] ) # Launch the interface demo.launch() Try It Yourself: Create demos for different AI tasks like image classification, translation, or text generation. Customize the interface with themes, additional inputs/outputs, and more advanced components. Deploy your demo to Hugging Face Spaces to share it with the community."
  },"/huggingface-tutorial/hugging%20face%20ecosystem%20tools/2025-03-09-datasets-library.html": {
    "title": "Datasets Library",
    "keywords": "Hugging Face Ecosystem Tools",
    "url": "/huggingface-tutorial/hugging%20face%20ecosystem%20tools/2025-03-09-datasets-library.html",
    "body": "The datasets library provides a unified interface for accessing and processing datasets for machine learning. Hands-on Example: Working with the Datasets Library from datasets import load_dataset, load_metric import matplotlib.pyplot as plt import numpy as np # Load a dataset dataset = load_dataset(\"glue\", \"sst2\") print(f\"Dataset structure: {dataset}\") # Explore dataset splits for split in dataset: print(f\"Split: {split}, Number of examples: {len(dataset[split])}\") # Look at sample data print(\"\\nSample examples from the training set:\") for i, example in enumerate(dataset[\"train\"][:5]): print(f\"Example {i+1}:\") print(f\" Text: {example['sentence']}\") print(f\" Label: {example['label']} ({dataset['train'].features['label'].names[example['label']]})\") # Dataset statistics sentence_lengths = [len(example[\"sentence\"].split()) for example in dataset[\"train\"]] plt.figure(figsize=(10, 6)) plt.hist(sentence_lengths, bins=50) plt.title(\"Distribution of Sentence Lengths in SST-2 Training Set\") plt.xlabel(\"Number of Words\") plt.ylabel(\"Frequency\") plt.tight_layout() plt.show() print(f\"Average sentence length: {np.mean(sentence_lengths):.2f} words\") print(f\"Median sentence length: {np.median(sentence_lengths):.2f} words\") print(f\"Min sentence length: {min(sentence_lengths)} words\") print(f\"Max sentence length: {max(sentence_lengths)} words\") # Class distribution labels = [example[\"label\"] for example in dataset[\"train\"]] label_counts = np.bincount(labels) label_names = dataset[\"train\"].features[\"label\"].names plt.figure(figsize=(8, 6)) plt.bar(label_names, label_counts) plt.title(\"Class Distribution in SST-2 Training Set\") plt.xlabel(\"Sentiment\") plt.ylabel(\"Count\") plt.tight_layout() plt.show() for i, name in enumerate(label_names): print(f\"Class {name}: {label_counts[i]} examples ({label_counts[i]/len(labels)*100:.2f}%)\") Try It Yourself: Explore different datasets for various tasks (e.g., imdb for sentiment analysis, squad for question answering). Create a custom dataset from your own data and share it on the Hugging Face Hub. Use dataset transformations like filtering, mapping, and shuffling to preprocess data for training."
  },"/huggingface-tutorial/domain-specific%20applications/2025-03-09-legal-text-processing.html": {
    "title": "Legal Text Processing",
    "keywords": "Domain-Specific Applications",
    "url": "/huggingface-tutorial/domain-specific%20applications/2025-03-09-legal-text-processing.html",
    "body": "Specialized models for legal documents: from transformers import pipeline # Legal document classification legal_classifier = pipeline( \"zero-shot-classification\" ) legal_documents = [ \"The parties hereby agree to arbitrate all disputes arising under this agreement.\", \"Tenant shall maintain liability insurance in the amount of $1,000,000.\", \"This agreement shall be governed by the laws of the State of New York.\" ] categories = [\"Arbitration Clause\", \"Insurance Requirement\", \"Governing Law\", \"Termination Provision\"] for doc in legal_documents: result = legal_classifier(doc, categories) print(f\"Text: {doc}\") print(f\"Classification: {result['labels'][0]} (Score: {result['scores'][0]:.4f})\") print(\"-\" * 50)"
  },"/huggingface-tutorial/domain-specific%20applications/2025-03-09-financial-text-analysis.html": {
    "title": "Financial Text Analysis",
    "keywords": "Domain-Specific Applications",
    "url": "/huggingface-tutorial/domain-specific%20applications/2025-03-09-financial-text-analysis.html",
    "body": "Models specialized for financial sentiment and document analysis: from transformers import AutoModelForSequenceClassification, AutoTokenizer import torch import torch.nn.functional as F # Load FinBERT for financial sentiment analysis model_name = \"ProsusAI/finbert\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name) # Analyze financial sentiment texts = [ \"The company reported a 50% increase in quarterly profits.\", \"The stock plummeted following the earnings miss.\", \"Analysts remain neutral on the company's growth prospects.\" ] for text in texts: inputs = tokenizer(text, return_tensors=\"pt\") with torch.no_grad(): outputs = model(**inputs) probs = F.softmax(outputs.logits, dim=1) labels = [\"negative\", \"neutral\", \"positive\"] sentiment = labels[probs.argmax().item()] confidence = probs.max().item() print(f\"Text: {text}\") print(f\"Sentiment: {sentiment} (confidence: {confidence:.4f})\") print(\"-\" * 50)"
  },"/huggingface-tutorial/domain-specific%20applications/2025-03-09-healthcare.html": {
    "title": "Healthcare and Biomedical NLP",
    "keywords": "Domain-Specific Applications",
    "url": "/huggingface-tutorial/domain-specific%20applications/2025-03-09-healthcare.html",
    "body": "Specialized models for processing biomedical texts: from transformers import AutoTokenizer, AutoModel, pipeline # Load BioBERT, a biomedical language model tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\") model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\") # Use BioBERT for biomedical named entity recognition ner = pipeline(\"ner\", model=\"drAbreu/bioBERT-NER-NCBI-disease\") text = \"Patients with diabetes mellitus often develop hypertension and coronary heart disease.\" entities = ner(text) print(\"Biomedical entities:\") for entity in entities: if entity['entity'].startswith('B-') or entity['entity'].startswith('I-'): print(f\"• {entity['word']} - {entity['entity'][2:]} (Score: {entity['score']:.4f})\") # Biomedical question answering qa_pipeline = pipeline( \"question-answering\", model=\"ktrapeznikov/biobert-v1.1-pubmed-squad-v2\" ) context = \"\"\" Coronavirus disease 2019 (COVID-19) is caused by SARS-CoV-2. The virus first identified in Wuhan, China, has spread globally, resulting in the ongoing COVID-19 pandemic. Common symptoms include fever, cough, fatigue, shortness of breath, and loss of smell and taste. \"\"\" question = \"What causes COVID-19?\" result = qa_pipeline(question=question, context=context) print(f\"\\nQuestion: {question}\") print(f\"Answer: {result['answer']} (Score: {result['score']:.4f})\")"
  },"/huggingface-tutorial/advanced%20topics/2025-03-09-deploying-models.html": {
    "title": "Deploying Models",
    "keywords": "Advanced Topics",
    "url": "/huggingface-tutorial/advanced%20topics/2025-03-09-deploying-models.html",
    "body": "Deployment makes your models available for use in applications, either locally or in the cloud. Hands-on Example: Creating a Simple REST API from transformers import pipeline from flask import Flask, request, jsonify # Initialize sentiment analysis pipeline sentiment_pipeline = pipeline(\"sentiment-analysis\") # Create a Flask app app = Flask(__name__) @app.route('/analyze', methods=['POST']) def analyze_sentiment(): # Get text from the request data = request.json if 'text' not in data: return jsonify({'error': 'No text provided'}), 400 # Analyze sentiment result = sentiment_pipeline(data['text'])[0] # Return result return jsonify({ 'text': data['text'], 'sentiment': result['label'], 'score': result['score'] }) # Example of how to start the server if __name__ == '__main__': print(\"Starting sentiment analysis API...\") print(\"Example usage:\") print(\"curl -X POST http://localhost:5000/analyze -H \\\"Content-Type: application/json\\\" -d '{\\\"text\\\":\\\"I love this product!\\\"}'\") app.run(debug=True) This example shows how to create a simple REST API for sentiment analysis using Flask. In a real-world scenario, you might use more robust frameworks like FastAPI and deploy to cloud platforms like AWS, Google Cloud, or Azure. Try It Yourself: Extend the API to support multiple NLP tasks (e.g., summarization, translation). Add input validation, error handling, and rate limiting for a more robust API. Deploy the API to a cloud platform or use Hugging Face Spaces for easy sharing."
  },"/huggingface-tutorial/advanced%20topics/2025-03-09-model-optimization.html": {
    "title": "Model Optimization Techniques",
    "keywords": "Advanced Topics",
    "url": "/huggingface-tutorial/advanced%20topics/2025-03-09-model-optimization.html",
    "body": "Optimization techniques help make models more efficient in terms of size, speed, and memory usage. Hands-on Example: Quantizing a Model from transformers import AutoModelForSequenceClassification, AutoTokenizer import torch from torch.quantization import quantize_dynamic import time # Load a pre-trained model model_name = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name) # Prepare sample input sample_text = \"I really enjoyed this movie. The acting was superb and the plot was engaging.\" inputs = tokenizer(sample_text, return_tensors=\"pt\") # Function to measure inference time def measure_inference_time(model, inputs, num_runs=100): start_time = time.time() for _ in range(num_runs): with torch.no_grad(): outputs = model(**inputs) end_time = time.time() return (end_time - start_time) / num_runs # Measure original model performance original_size = sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024 # Size in MB original_time = measure_inference_time(model, inputs) print(f\"Original model size: {original_size:.2f} MB\") print(f\"Original model average inference time: {original_time*1000:.2f} ms\") # Quantize the model quantized_model = quantize_dynamic( model, {torch.nn.Linear}, dtype=torch.qint8 ) # Measure quantized model performance quantized_size = sum(p.numel() for p in quantized_model.parameters()) * 1 / 1024 / 1024 # Approximation quantized_time = measure_inference_time(quantized_model, inputs) print(f\"Quantized model size: {quantized_size:.2f} MB\") print(f\"Quantized model average inference time: {quantized_time*1000:.2f} ms\") print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.2f}%\") print(f\"Speed improvement: {(1 - quantized_time/original_time)*100:.2f}%\") # Check accuracy with torch.no_grad(): original_output = model(**inputs).logits quantized_output = quantized_model(**inputs).logits original_prediction = torch.argmax(original_output, dim=1).item() quantized_prediction = torch.argmax(quantized_output, dim=1).item() print(f\"Original model prediction: {original_prediction}\") print(f\"Quantized model prediction: {quantized_prediction}\") print(f\"Predictions match: {original_prediction == quantized_prediction}\") This example demonstrates dynamic quantization, which reduces model size and improves inference speed with minimal impact on accuracy. Try It Yourself: Try quantization with different models and tasks to see how it affects performance. Experiment with other optimization techniques like pruning and knowledge distillation. Use the optimum library from Hugging Face for more advanced optimization techniques."
  },"/huggingface-tutorial/advanced%20topics/2025-03-09-finetuning-models.html": {
    "title": "Finetuning Models",
    "keywords": "Advanced Topics",
    "url": "/huggingface-tutorial/advanced%20topics/2025-03-09-finetuning-models.html",
    "body": "Fine-tuning allows you to adapt pre-trained models to your specific data and tasks, often leading to better performance. Hands-on Example: Fine-tuning a Text Classification Model from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments from datasets import load_dataset, Dataset import pandas as pd import numpy as np from sklearn.metrics import accuracy_score, precision_recall_fscore_support import torch # Load a small dataset (IMDB reviews subset in this example) dataset = load_dataset(\"imdb\", split=\"train[:1000]\") # Split into train and validation dataset = dataset.train_test_split(test_size=0.2) train_dataset = dataset[\"train\"] eval_dataset = dataset[\"test\"] # Load pre-trained model and tokenizer model_name = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Tokenize data def tokenize_function(examples): return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True) tokenized_train = train_dataset.map(tokenize_function, batched=True) tokenized_eval = eval_dataset.map(tokenize_function, batched=True) # Define metrics function def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary') acc = accuracy_score(labels, preds) return { 'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall } # Define training arguments training_args = TrainingArguments( output_dir=\"./results\", num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=16, warmup_steps=500, weight_decay=0.01, logging_dir=\"./logs\", logging_steps=10, evaluation_strategy=\"epoch\", ) # Initialize Trainer trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_eval, compute_metrics=compute_metrics, ) # Fine-tune the model trainer.train() # Evaluate the model eval_results = trainer.evaluate() print(\"Evaluation results:\", eval_results) # Save fine-tuned model and tokenizer model.save_pretrained(\"./fine-tuned-model\") tokenizer.save_pretrained(\"./fine-tuned-model\") # Test the fine-tuned model on a new example test_text = \"This movie was absolutely fantastic! I loved every minute of it.\" inputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True) outputs = model(**inputs) predicted_class = torch.argmax(outputs.logits, dim=1).item() print(f\"Test text: {test_text}\") print(f\"Predicted class: {'Positive' if predicted_class == 1 else 'Negative'}\") This example demonstrates a basic fine-tuning workflow for a text classification model, including data preparation, training, evaluation, and inference. Try It Yourself: Try fine-tuning on your own dataset by creating a custom Dataset from a CSV or JSON file. Experiment with different model architectures like BERT, RoBERTa, or DistilBERT. Try different hyperparameters like learning rate, batch size, and number of epochs to optimize performance."
  },"/huggingface-tutorial/multimodal%20applications/2025-03-09-contrastive-lang-image-pretraining.html": {
    "title": "CLIP - Bridging Vision and Language",
    "keywords": "Multimodal Applications",
    "url": "/huggingface-tutorial/multimodal%20applications/2025-03-09-contrastive-lang-image-pretraining.html",
    "body": "CLIP (Contrastive Language-Image Pre-training) is a powerful model that understands both images and text in a shared embedding space, enabling various multimodal tasks. Hands-on Example: Image-Text Similarity with CLIP from transformers import CLIPProcessor, CLIPModel from PIL import Image import requests from io import BytesIO import torch import matplotlib.pyplot as plt import numpy as np # Load CLIP model and processor model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\") processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\") # Load an image image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\" # Cat image response = requests.get(image_url) image = Image.open(BytesIO(response.content)) # Display the image plt.figure(figsize=(8, 8)) plt.imshow(image) plt.axis('off') plt.title(\"Query Image\") plt.show() # Define text descriptions to compare against texts = [ \"a photo of a cat\", \"a photo of a dog\", \"a photo of a pizza\", \"a photo of a sunset\", \"a drawing of a cat\", \"a close-up photo of a cat\", \"a black and white photo of a cat\", ] # Compute image-text similarity inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True) with torch.no_grad(): outputs = model(**inputs) logits_per_image = outputs.logits_per_image probs = logits_per_image.softmax(dim=1) # Print results print(\"Image-text similarity scores:\") for i, text in enumerate(texts): print(f\"'{text}': {probs[0][i].item():.4f}\") # Visualize results plt.figure(figsize=(10, 6)) plt.bar(range(len(texts)), probs[0].numpy()) plt.xticks(range(len(texts)), [t[:15] + \"...\" if len(t) &gt; 15 else t for t in texts], rotation=45, ha=\"right\") plt.title(\"Image-Text Similarity Scores\") plt.tight_layout() plt.show() CLIP can be used for various tasks such as zero-shot image classification, image retrieval, and visual search, all without task-specific fine-tuning. Try It Yourself: Use CLIP for zero-shot image classification by comparing an image against a list of category descriptions. Create an image retrieval system that finds the most relevant image for a text query. Experiment with CLIP’s understanding of visual concepts and relationships."
  },"/huggingface-tutorial/multimodal%20applications/2025-03-09-image-captioning.html": {
    "title": "Image Captioning",
    "keywords": "Multimodal Applications",
    "url": "/huggingface-tutorial/multimodal%20applications/2025-03-09-image-captioning.html",
    "body": "Image captioning generates descriptive text for images, useful for accessibility and content indexing. Hands-on Example: Generating Captions for Images from transformers import pipeline from PIL import Image import requests from io import BytesIO import matplotlib.pyplot as plt # Initialize the image-to-text pipeline image_captioner = pipeline(\"image-to-text\") # Load images from URLs image_urls = [ \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/San_Francisco_skyline_at_night_from_Pier_7.jpg/800px-San_Francisco_skyline_at_night_from_Pier_7.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Giraffe_at_Kruger_National_Park%2C_South_Africa_%28square_crop%29.jpg/800px-Giraffe_at_Kruger_National_Park%2C_South_Africa_%28square_crop%29.jpg\" ] # Generate captions for each image for i, url in enumerate(image_urls): # Load image response = requests.get(url) image = Image.open(BytesIO(response.content)) # Display image plt.figure(figsize=(8, 8)) plt.imshow(image) plt.axis('off') plt.title(f\"Image {i+1}\") plt.show() # Generate caption captions = image_captioner(image) print(f\"Generated captions for Image {i+1}:\") for caption in captions: print(f\"• {caption['generated_text']}\") print(\"-\" * 50) The image captioning pipeline generates descriptive text for images, demonstrating how vision and language models can be combined. Try It Yourself: Generate captions for personal photos or artwork to see how the model interprets different visual styles. Try different models like nlpconnect/vit-gpt2-image-captioning for comparison. Test the captioning on abstract or ambiguous images to see how the model handles them."
  },"/huggingface-tutorial/multimodal%20applications/2025-03-09-visual-question-answering.html": {
    "title": "Visual Question Answering",
    "keywords": "Multimodal Applications",
    "url": "/huggingface-tutorial/multimodal%20applications/2025-03-09-visual-question-answering.html",
    "body": "Visual Question Answering (VQA) answers questions about images, combining computer vision and natural language processing. Hands-on Example: Answering Questions About Images from transformers import pipeline from PIL import Image import requests from io import BytesIO import matplotlib.pyplot as plt # Initialize the visual question answering pipeline vqa = pipeline(\"visual-question-answering\") # Load an image image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/800px-Cute_dog.jpg\" response = requests.get(image_url) image = Image.open(BytesIO(response.content)) # Questions about the image questions = [ \"What animal is in the image?\", \"What color is the dog?\", \"Is the dog inside or outside?\", \"Does the dog look happy?\" ] # Display the image plt.figure(figsize=(8, 8)) plt.imshow(image) plt.axis('off') plt.title(\"Query Image\") plt.show() # Answer each question print(\"Visual Question Answering:\") for question in questions: result = vqa(image=image, question=question) print(f\"Q: {question}\") print(f\"A: {result['answer']} (Score: {result['score']:.4f})\") print(\"-\" * 50) The visual question answering pipeline combines image understanding with language comprehension to answer questions about visual content. Try It Yourself: Test VQA on complex scenes with multiple objects and ask questions about relationships between objects. Try asking more abstract questions about mood, style, or aesthetic qualities. Experiment with ambiguous questions to see how the model handles uncertainty."
  },"/huggingface-tutorial/speech%20processing/2025-03-09-text-to-speech.html": {
    "title": "Text-to-Speech",
    "keywords": "Speech Processing",
    "url": "/huggingface-tutorial/speech%20processing/2025-03-09-text-to-speech.html",
    "body": "Text-to-speech (TTS) converts written text into spoken words, enabling applications like screen readers and voice assistants. Hands-on Example: Generating Speech from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan import torch import soundfile as sf from datasets import load_dataset import IPython.display as ipd # Load processor, model and vocoder processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\") model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\") vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\") # Get speaker embeddings from a dataset embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\") speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0) # Text to synthesize texts = [ \"Welcome to Hugging Face! This is a demonstration of text to speech synthesis.\", \"Artificial intelligence is transforming how we interact with technology.\", \"Machine learning models can now generate realistic human speech.\" ] # Synthesize speech for each text for i, text in enumerate(texts): # Process text inputs = processor(text=text, return_tensors=\"pt\") # Generate speech speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder) # Save audio output_file = f\"synthesized_speech_{i+1}.wav\" sf.write(output_file, speech.numpy(), samplerate=16000) print(f\"Generated speech for: '{text}'\") # In a notebook, you could play the audio with: # ipd.display(ipd.Audio(output_file)) The SpeechT5 model converts text to natural-sounding speech, demonstrating how Hugging Face models can be used for audio synthesis. Try It Yourself: Generate speech in different styles by trying different speaker embeddings. Experiment with text that includes questions, exclamations, or different emotions. Try the facebook/fastspeech2-en-ljspeech model for comparison."
  },"/huggingface-tutorial/speech%20processing/2025-03-09-audio-classification.html": {
    "title": "Audio Classification",
    "keywords": "Speech Processing",
    "url": "/huggingface-tutorial/speech%20processing/2025-03-09-audio-classification.html",
    "body": "Audio classification identifies sounds or categorizes audio clips based on their content. Hands-on Example: Classifying Audio from transformers import pipeline import librosa import soundfile as sf import matplotlib.pyplot as plt import numpy as np import requests # Initialize the audio classification pipeline audio_classifier = pipeline(\"audio-classification\") # Download audio samples audio_urls = { \"dog\": \"https://github.com/librosa/librosa/raw/main/tests/data/choice.wav\", # Using as placeholder \"siren\": \"https://github.com/librosa/librosa/raw/main/tests/data/choice.wav\", # Using as placeholder \"piano\": \"https://github.com/librosa/librosa/raw/main/tests/data/choice.wav\" # Using as placeholder } # Process each audio file for label, url in audio_urls.items(): # Download and save response = requests.get(url) filename = f\"{label}_sound.wav\" with open(filename, \"wb\") as f: f.write(response.content) # Classify audio results = audio_classifier(filename) # Display top 3 predictions print(f\"Audio: {filename}\") for result in results[:3]: print(f\"• {result['label']}: {result['score']:.4f}\") print(\"-\" * 50) # Visualize audio waveform audio, sr = librosa.load(filename, sr=16000) plt.figure(figsize=(10, 4)) plt.plot(np.linspace(0, len(audio)/sr, len(audio)), audio) plt.title(f\"Waveform for {label} sound\") plt.xlabel(\"Time (s)\") plt.ylabel(\"Amplitude\") plt.tight_layout() plt.show() The audio classification pipeline identifies the type of sound in an audio clip, useful for applications like environmental sound recognition and content moderation. Try It Yourself: Classify different types of music or environmental sounds. Try the facebook/wav2vec2-base-960h model for potentially better performance. Create mixed audio samples and see how the classifier performs on more complex inputs."
  },"/huggingface-tutorial/speech%20processing/2025-03-09-speech-recognition.html": {
    "title": "Speech Recognition",
    "keywords": "Speech Processing",
    "url": "/huggingface-tutorial/speech%20processing/2025-03-09-speech-recognition.html",
    "body": "Speech recognition (also known as automatic speech recognition or ASR) converts spoken language into written text. Hands-on Example: Transcribing Speech from transformers import pipeline import librosa import soundfile as sf import matplotlib.pyplot as plt import numpy as np import requests from io import BytesIO # Initialize the automatic speech recognition pipeline transcriber = pipeline(\"automatic-speech-recognition\") # Download an audio sample audio_url = \"https://github.com/librosa/librosa/raw/main/tests/data/choice.wav\" response = requests.get(audio_url) with open(\"speech_sample.wav\", \"wb\") as f: f.write(response.content) # Load the audio audio, sr = librosa.load(\"speech_sample.wav\", sr=16000) # Visualize the waveform plt.figure(figsize=(10, 4)) plt.plot(np.linspace(0, len(audio)/sr, len(audio)), audio) plt.title(\"Audio Waveform\") plt.xlabel(\"Time (s)\") plt.ylabel(\"Amplitude\") plt.tight_layout() plt.show() # Transcribe audio result = transcriber(\"speech_sample.wav\") print(f\"Transcription: {result['text']}\") The speech recognition pipeline converts audio recordings into text, using pre-trained models that have been fine-tuned on large datasets of speech. Try It Yourself: Record your own voice using a tool like Audacity and transcribe it. Try transcribing audio in different languages using models like facebook/wav2vec2-large-960h-lv60-self. Experiment with audio that has background noise or multiple speakers to test model robustness."
  },"/huggingface-tutorial/computer%20vision/2025-03-09-image-generation.html": {
    "title": "Image Generation",
    "keywords": "Computer Vision",
    "url": "/huggingface-tutorial/computer%20vision/2025-03-09-image-generation.html",
    "body": "Image generation creates new images based on text descriptions, enabling creative applications and content creation. Hands-on Example: Text-to-Image Generation with Diffusers from diffusers import StableDiffusionPipeline import torch import matplotlib.pyplot as plt # Initialize the Stable Diffusion pipeline (requires about 7GB of VRAM) # This will download a large model (~4GB) on first run pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16) # Move to GPU if available device = \"cuda\" if torch.cuda.is_available() else \"cpu\" pipe = pipe.to(device) # Generate images from prompts prompts = [ \"A serene landscape with mountains and a lake at sunset\", \"A futuristic city with flying cars and tall skyscrapers\", \"A cute robot playing with a kitten in a garden\" ] # Create figure for displaying results plt.figure(figsize=(15, 5 * len(prompts))) for i, prompt in enumerate(prompts): print(f\"Generating: {prompt}\") image = pipe(prompt).images[0] # Display image plt.subplot(len(prompts), 1, i+1) plt.imshow(image) plt.title(prompt) plt.axis('off') plt.tight_layout() plt.show() The Stable Diffusion pipeline generates images based on text descriptions, demonstrating the power of text-to-image models. Try It Yourself: Create detailed prompts that specify style, content, and mood for more controlled generation. Experiment with different models like CompVis/stable-diffusion-v1-4 or stabilityai/stable-diffusion-2-1. Try adjusting parameters like guidance_scale and num_inference_steps to control the generation process."
  },"/huggingface-tutorial/computer%20vision/2025-03-09-image-segmentation.html": {
    "title": "Image Segmentation",
    "keywords": "Computer Vision",
    "url": "/huggingface-tutorial/computer%20vision/2025-03-09-image-segmentation.html",
    "body": "Image segmentation assigns a class to each pixel in the image, providing more detailed information than bounding boxes. Hands-on Example: Semantic Segmentation from transformers import pipeline from PIL import Image import requests from io import BytesIO import matplotlib.pyplot as plt import numpy as np # Initialize the image segmentation pipeline segmenter = pipeline(\"image-segmentation\") # Load an image image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Busy_street_in_Delhi.jpg/800px-Busy_street_in_Delhi.jpg\" response = requests.get(image_url) image = Image.open(BytesIO(response.content)) # Segment the image results = segmenter(image) # Display original image plt.figure(figsize=(15, 8)) plt.subplot(1, 2, 1) plt.imshow(image) plt.title(\"Original Image\") plt.axis('off') # Create segmentation visualization # Get unique masks for overlay plt.subplot(1, 2, 2) plt.imshow(image) plt.title(\"Segmentation Overlay\") plt.axis('off') # Generate random colors for each segment np.random.seed(42) colors = np.random.randint(0, 255, size=(len(results), 3)) / 255.0 # Add semi-transparent overlays for i, segment in enumerate(results): mask = segment['mask'].convert('L') mask_array = np.array(mask) # Create colored mask colored_mask = np.zeros((mask_array.shape[0], mask_array.shape[1], 4)) colored_mask[mask_array &gt; 0] = [*colors[i], 0.4] # Add alpha channel plt.imshow(colored_mask) # Print detected segments print(\"Segmented parts:\") for segment in results: print(f\"• {segment['label']} (score: {segment.get('score', 'N/A')})\") plt.tight_layout() plt.show() The image segmentation pipeline identifies regions in the image and classifies each pixel, creating a detailed map of the image content. Try It Yourself: Apply segmentation to landscape images to see how it identifies terrain features. Try different models like facebook/detr-resnet-50-panoptic for panoptic segmentation (which distinguishes individual instances). Experiment with different visualization techniques for the segmentation masks."
  },"/huggingface-tutorial/computer%20vision/2025-03-09-object-detection.html": {
    "title": "Object Detection",
    "keywords": "Computer Vision",
    "url": "/huggingface-tutorial/computer%20vision/2025-03-09-object-detection.html",
    "body": "Object detection identifies and localizes multiple objects within an image, providing bounding boxes around them. Hands-on Example: Detecting Objects from transformers import pipeline from PIL import Image, ImageDraw, ImageFont import requests from io import BytesIO import matplotlib.pyplot as plt # Initialize the object detection pipeline object_detector = pipeline(\"object-detection\") # Load an image image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Hotdog_with_mustard.png/800px-Hotdog_with_mustard.png\" response = requests.get(image_url) image = Image.open(BytesIO(response.content)) # Detect objects results = object_detector(image) # Draw bounding boxes draw = ImageDraw.Draw(image) for result in results: box = result[\"box\"] label = f\"{result['label']}: {result['score']:.2f}\" # Draw rectangle draw.rectangle([(box[\"xmin\"], box[\"ymin\"]), (box[\"xmax\"], box[\"ymax\"])], outline=\"red\", width=3) # Draw label draw.text((box[\"xmin\"], box[\"ymin\"] - 10), label, fill=\"red\") # Display image with detections plt.figure(figsize=(10, 10)) plt.imshow(image) plt.axis('off') plt.show() # Also print detection results print(\"Detected objects:\") for result in results: print(f\"• {result['label']} with confidence {result['score']:.4f} at position {result['box']}\") The object detection pipeline identifies objects in the image and provides their bounding box coordinates and confidence scores. Try It Yourself: Test object detection on images with multiple objects, like street scenes or group photos. Try different models like facebook/detr-resnet-50 for potentially better performance. Experiment with detection threshold by filtering results: [r for r in results if r['score'] &gt; 0.7]."
  },"/huggingface-tutorial/computer%20vision/2025-03-09-image-classification.html": {
    "title": "Image Classification",
    "keywords": "Computer Vision",
    "url": "/huggingface-tutorial/computer%20vision/2025-03-09-image-classification.html",
    "body": "Image classification assigns labels to entire images, identifying what they primarily depict. Hands-on Example: Classifying Images from transformers import pipeline from PIL import Image import requests from io import BytesIO import matplotlib.pyplot as plt # Initialize the image classification pipeline image_classifier = pipeline(\"image-classification\") # Load images from URLs image_urls = [ \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Collage_of_Nine_Dogs.jpg/800px-Collage_of_Nine_Dogs.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Cat_November_2010-1a.jpg/767px-Cat_November_2010-1a.jpg\", \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/F-15E_Strike_Eagle.jpg/800px-F-15E_Strike_Eagle.jpg\" ] # Classify each image for url in image_urls: # Load image response = requests.get(url) image = Image.open(BytesIO(response.content)) # Classify results = image_classifier(image) # Display top 3 predictions print(f\"Image: {url.split('/')[-1]}\") for result in results[:3]: print(f\"• {result['label']}: {result['score']:.4f}\") print(\"-\" * 50) The image classification pipeline predicts what’s depicted in the image, providing labels and confidence scores. Try It Yourself: Classify images from your own collection by loading them from disk: image = Image.open(\"path/to/image.jpg\"). Try different pre-trained models like google/vit-base-patch16-224 or microsoft/resnet-50 by specifying them in the pipeline. See how the model performs on ambiguous images or images with multiple subjects."
  },"/huggingface-tutorial/natural%20language%20processing/2025-03-09-named-entity-recognition.html": {
    "title": "Named Entity Recognition",
    "keywords": "Natural Language Processing",
    "url": "/huggingface-tutorial/natural%20language%20processing/2025-03-09-named-entity-recognition.html",
    "body": "Named Entity Recognition (NER) identifies entities like people, organizations, locations, dates, and more in text. Hands-on Example: Detecting Entities from transformers import pipeline # Initialize the NER pipeline ner = pipeline(\"ner\", aggregation_strategy=\"simple\") # Text with entities text = \"\"\" Apple Inc. is planning to open a new office in Berlin by January 2026. CEO Tim Cook announced this during his visit to Germany last week, where he met with Chancellor Olaf Scholz. The company plans to invest about $500 million in this expansion. \"\"\" # Identify entities entities = ner(text) # Display results print(\"Identified entities:\") for entity in entities: print(f\"• {entity['word']} - {entity['entity_group']} (Confidence: {entity['score']:.4f})\") The NER pipeline identifies entities in the text and classifies them into categories like person (PER), organization (ORG), location (LOC), and date/time expressions (DATE). Try It Yourself: Apply NER to a news article and see what entities are detected. Try texts in different domains (science, sports, politics) to see how entity detection varies. Experiment with different models like dbmdz/bert-large-cased-finetuned-conll03-english for potentially improved performance."
  },"/huggingface-tutorial/natural%20language%20processing/2025-03-09-text-summarization.html": {
    "title": "Text Summarization",
    "keywords": "Natural Language Processing",
    "url": "/huggingface-tutorial/natural%20language%20processing/2025-03-09-text-summarization.html",
    "body": "Summarization condenses a longer text into a shorter version while preserving key information. Hands-on Example: Abstractive Summarization from transformers import pipeline # Initialize the summarization pipeline summarizer = pipeline(\"summarization\") # Text to summarize article = \"\"\" Artificial intelligence (AI) is revolutionizing industries worldwide. From healthcare to finance, AI systems are improving efficiency and enabling new possibilities. In healthcare, AI assists in diagnosing diseases from medical images with accuracy rivaling human doctors. Financial institutions use AI for fraud detection, risk assessment, and algorithmic trading. Transportation is being transformed through autonomous vehicles which promise to reduce accidents and congestion. Manufacturing benefits from predictive maintenance and quality control powered by AI. Despite these advances, concerns about job displacement, algorithm bias, and privacy remain. Researchers and policymakers are working to address these challenges while maximizing AI's positive impact. The future of AI involves more sophisticated models, enhanced human-AI collaboration, and wider deployment across industries. \"\"\" # Generate summary summary = summarizer(article, max_length=100, min_length=30, do_sample=False) print(\"Original text:\") print(article) print(\"\\nSummary:\") print(summary[0]['summary_text']) The summarization pipeline creates a concise version of the input text, focusing on the most important points. Try It Yourself: Summarize a news article or research paper abstract. Experiment with different max_length and min_length values to control summary length. Try different models like facebook/bart-large-cnn or google/pegasus-xsum for different summarization styles."
  },"/huggingface-tutorial/natural%20language%20processing/2025-03-09-text-generation.html": {
    "title": "Text Generation",
    "keywords": "Natural Language Processing",
    "url": "/huggingface-tutorial/natural%20language%20processing/2025-03-09-text-generation.html",
    "body": "Text generation involves creating coherent text based on a prompt. This is useful for applications like chatbots, content creation, and creative writing assistance. Hands-on Example: Text Completion from transformers import pipeline # Initialize the text generation pipeline generator = pipeline(\"text-generation\", model=\"gpt2\") # Generate text from prompts prompts = [ \"In the distant future, artificial intelligence\", \"The best way to learn programming is\", \"Climate change has affected many regions, leading to\" ] # Generate and display completions for prompt in prompts: completions = generator(prompt, max_length=50, num_return_sequences=2) print(f\"Prompt: {prompt}\") for i, completion in enumerate(completions): print(f\"Completion {i+1}: {completion['generated_text']}\") print(\"-\" * 50) The text generation pipeline continues text from the given prompts, producing creative and contextually relevant completions. The default model is GPT-2, but you can specify other models as needed. Try It Yourself: Experiment with different prompts related to your interests. Try adjusting parameters like max_length, num_return_sequences, and temperature (controls randomness). Use different models like EleutherAI/gpt-neo-1.3B for potentially better completions."
  },"/huggingface-tutorial/natural%20language%20processing/2025-03-09-question-answering.html": {
    "title": "Question Answering",
    "keywords": "Natural Language Processing",
    "url": "/huggingface-tutorial/natural%20language%20processing/2025-03-09-question-answering.html",
    "body": "Question answering systems respond to queries in natural language with relevant answers, often extracted from a given context. Hands-on Example: Extractive Question Answering from transformers import pipeline # Initialize the question answering pipeline qa_pipeline = pipeline(\"question-answering\") # Context and question context = \"\"\" Hugging Face is an AI community and platform that provides tools to build, train, and deploy machine learning models. Founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf, the company has grown to become a leading provider of open-source tools in natural language processing. Their Transformers library has over 50,000 GitHub stars and supports frameworks like PyTorch, TensorFlow, and JAX. In 2021, Hugging Face raised $100 million in a Series C funding round. \"\"\" questions = [ \"Who founded Hugging Face?\", \"What is Hugging Face?\", \"How much funding did Hugging Face raise in 2021?\" ] # Get answers for question in questions: result = qa_pipeline(question=question, context=context) print(f\"Question: {question}\") print(f\"Answer: {result['answer']}\") print(f\"Confidence: {result['score']:.4f}\") print(\"-\" * 50) The question answering pipeline identifies spans in the context that answer the given questions, along with a confidence score. Try It Yourself: Use a longer passage from a Wikipedia article as context and ask questions about it. Try questions whose answers aren’t directly in the text. How does the model respond? Experiment with different models, such as deepset/roberta-base-squad2 for improved performance."
  },"/huggingface-tutorial/natural%20language%20processing/2025-03-09-text-classification.html": {
    "title": "Text Classification",
    "keywords": "Natural Language Processing",
    "url": "/huggingface-tutorial/natural%20language%20processing/2025-03-09-text-classification.html",
    "body": "Text classification is the task of assigning predefined categories to text documents. Common applications include sentiment analysis, topic classification, and language detection. Hands-on Example: Sentiment Analysis from transformers import pipeline # Initialize the sentiment analysis pipeline sentiment_analyzer = pipeline(\"sentiment-analysis\") # Analyze text texts = [ \"I absolutely loved this movie! The acting was superb.\", \"The service at this restaurant was terrible and the food was bland.\", \"The weather today is okay, not great but not terrible either.\" ] # Get results results = sentiment_analyzer(texts) # Display results for text, result in zip(texts, results): print(f\"Text: {text}\") print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\") print(\"-\" * 50) When you run this code, you’ll see the sentiment (POSITIVE or NEGATIVE) and the confidence score for each text. The pipeline abstracts away many of the complexities, using a pre-trained model (by default, DistilBERT fine-tuned on a sentiment analysis dataset). Try It Yourself: Use the sentiment analysis pipeline on some reviews of your favorite products or movies. What happens if you analyze more ambiguous or neutral text? Try changing the model by specifying a different one: sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\") - this model provides more fine-grained sentiment (1-5 stars)."
  }}
